---
title: "Finding Thurstone: modeling comparative judgment data with `R` (and `Stan`)"
author:
  - name: 
      given: Jose Manuel
      family: Rivera Espejo
    orcid: 0000-0002-3088-2783
    url: https://www.uantwerpen.be/en/staff/jose-manuel-rivera-espejo_23166/
    email: JoseManuel.RiveraEspejo@uantwerpen.be
    corresponding: true
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Tine
      family: Daal
      non-dropping-particle: van
    orcid: 0000-0001-9398-9775
    url: https://www.uantwerpen.be/en/staff/tine-vandaal/
    email: tine.vandaal@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Sven
      family: Maeyer
      non-dropping-particle: De
    orcid: 0000-0003-2888-1631
    url: https://www.uantwerpen.be/en/staff/sven-demaeyer/
    email: sven.demaeyer@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name:
      given: Steven
      family: Gillis
    orcid: 
    url: https://www.uantwerpen.be/nl/personeel/steven-gillis/
    email: steven.gillis@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Linguistics
        group: Centre for computational linguistics, psycholinguistics, and sociolinguistics (CLiPS)
funding: 
  statement: "The project was founded through the Research Fund of the University of Antwerp (BOF)."
keywords:
  - tutorial
  - causal inference
  - bayesian inference
  - thurstonian model
  - comparative judgement
  - statistical modeling
abstract: |
  The classical BTL analysis has become the standard approach for analyzing comparative judgment (CJ) data because it provides a simple method for measuring traits and conducting related analysis. This simplicity arises from two key features. First, the approach relies on the Bradley-Terry-Luce (BTL) model to estimate latent traits. Second, it uses of ad hoc procedures to conduct related analysis. However, recent studies question whether the BTL model assumptions hold in contemporary CJ applications and whether the ad hoc procedures effectively fulfill their intended analytical goals.
  
  To address these concerns, Rivera and colleagues [-@Rivera_et_al_2025] proposed an approach that extends the general form of Thurstone's law of comparative judgment. The approach enables the development of a model tailored to the assumed data-generating process of the CJ system under study, eliminating the need for simplifying assumptions. Moreover, by integrating measurement and inference within a single analytical framework, it also removes the dependence on ad hoc analytical procedures. Despite these advantages, the approach still requires empirical validation.
  
  Thus, this study empirically validates the proposed Information-Theoretical model for CJ, benchmarked against the classical BTL analysis, and demonstrates its practical implementation. The document includes a structured tutorial based on a simulated speech-quality dataset, providing guidance on data simulation, prior specification, model estimation, and interpretation using `Stan`, `R`, and the interface packages `cmdstan` and `brms`. Ultimately, the study equips researchers with practical tools to apply the model to more complex CJ studies.
date: last-modified
bibliography: references.bib
---

<!-- ######################################### -->

# Introduction {#sec-introduction}
<!-- Sven and Tine 2025.08.25, time: 00:04:30 - 00:09:30 -->
<!-- Sven and Tine 2025.08.25, time: 00:10:25 - 00:10:50 -->
<!-- Sven and Tine 2025.08.25, time: 00:23:30 - 00:32:50 -->
<!-- Sven and Tine 2025.09.04, time: 00:28:45 - 00:30:20 -->
<!-- Sven and Tine 2025.09.15, time: 00:02:50 - 00:03:30 -->
<!-- Sven and Tine 2025.09.04, time: 00:00:00 - 00:04:05, perfect -->

<!-- 1. What is the CJ? -->
<!-- Sven and Tine 25.08.25, time: 00:04:30 - 00:09:30 -->
<!-- do not make it about speech quality, Put examples of many applications, add a couple more examples. Present it in a more broader way, e.g., "using speech quality as an example" -->
*Comparative judgment* (CJ) has emerged as a valuable methodology for measuring latent traits across diverse fields, including education [@Kimbell_2012; @Jones_et_al_2015; @vanDaal_et_al_2016; @Bartholomew_et_al_2018], political sciences [@Zucco_et_al_2019], linguistics [@Boonen_et_al_2020], and criminology [@Seymour_et_al_2025]. In CJ studies, judges actively compare pairs of stimuli to determine which stimulus exhibits more of the latent trait of interest [@Thurstone_1927a; @Thurstone_1927b].

<!-- 2. How does CJ data is analyzed? -->
A specific data analysis workflow has become the standard approach for analyzing CJ data [see, e.g., @Thwaites_et_al_2024]. In this study, we refer to this workflow as the classical BTL analysis or *CBTL analysis*. Researchers favor this approach because it provides a simple method for measuring traits and conducting related analyses [@Andrich_1978; @Pollitt_2012b]. This simplicity arises from two key features. First, the approach relies on the Bradley-Terry-Luce (BTL) model [@Bradley_et_al_1952; @Luce_1959] to estimate latent traits. The model facilitates trait estimation by imposing several simplifying assumptions about traits, judges, and stimuli present in CJ assessments [@Thurstone_1927b; @Bramley_2008]. Second, the approach uses ad hoc procedures to conduct related analysis, including data summarization and statistical inference [@Pollitt_2012b].

<!-- 3. But what is the problem with using the BTL model? -->
Recent studies, however, question whether the assumptions of the BTL model hold in contemporary CJ applications and whether the ad hoc procedures achieve their intended analytical goals [@Bramley_2008; @Kelly_et_al_2022; @Rivera_et_al_2025]. For instance, Rivera and colleagues [-@Rivera_et_al_2025] argue that while the assumptions of equal dispersions and zero correlations between stimuli simplify trait measurement, they may fail to represent complex traits or heterogeneous stimuli adequately [@Thurstone_1927a; @Andrich_1978; @vanDaal_et_al_2016; @Lesterhuis_et_al_2018; @Chambers_et_al_2022]. As a result, such assumptions can compromise the reliability and accuracy of trait estimates [@Ackerman_1989; @Zimmerman_1994; @McElreath_2020; @Wu_et_al_2022; @Miller_2023; @Hoyle_et_al_2023]. Moreover, the same authors note that although ad hoc procedures simplify data analyses, the use of untested methods can also undermine the validity of inferences derived from CJ data [@McElreath_2020; @Kline_et_al_2023; @Hoyle_et_al_2023].

<!-- 4. How it has been solved? -->
To address these concerns, @Rivera_et_al_2025 proposed an approach that extends the general form of Thurstone's law of comparative judgment [@Thurstone_1927a; @Thurstone_1927b], referred to as the Information-Theoretical model for CJ (hereafter, ITCJ analysis). This approach leverages causal and Bayesian inference methods to combine Thurstone's core theoretical principles with key design features of CJ assessment. By doing so, it enables the development of a model tailored to the assumed data-generating process of the CJ system under study. This tailoring effectively removes the need to rely on the simplifying assumptions of the BTL model. Moreover, by integrating measurement and inference within a single analytical framework, the approach also eliminates the dependence on ad hoc analytical procedures.


## Research goals {#sec-introduction_goals}
<!-- Sven and Tine 25.08.25, time: 00:10:25 - 00:10:50 -->
<!-- Sven and Tine 25.08.25, time: 00:23:30 - 00:32:50 -->
<!-- The way to present the document is: this document that aims to validate the model, and by the way this also allow us to guide researchers on how to use the model (a side product). -->
<!-- - Make sure you tell that this paper is a validation study -->
<!-- - Make sure to state what do researchers learn from the document -->
<!-- Sven and Tine 2025.09.04, time: 00:28:45 - 00:30:20 -->
<!-- The model stills need to be empirically tested, the ITCJ implies a complex models which put challenges to integrate it in the workflow -->

<!-- 1. Why we want to do it and what are we going to do-->
The ITCJ analysis [@Rivera_et_al_2025] shows theoretical promise for yielding reliable trait estimates and accurate statistical inferences. However, as noted by the authors, this promise has not yet been empirically validated. Thus, the present study addresses this gap by pursuing two closely related research goals. The first goal is to *empirically validate* the proposed ITCJ analysis by evaluating the accuracy and reliability of its trait estimates and inference parameters, benchmarked against the CBTL analysis. 

The second goal emerges as a practical byproduct of this validation: *to demonstrate how to implement the model in practice*. To this end, the document provides a structured tutorial based on a simulated speech-quality dataset, offering guidance on data simulation, prior specification, model estimation, and interpretation using `Stan`[@Stan_2026a; @Stan_2026b], `R` [@R_2015], and the interface packages `cmdstan` [@Gabry_et_al_2025b] and `brms` [@Burkner_2017; @Burkner_2018]. By combining model validation and practical instruction, the study evaluates the methodological performance of the ITCJ analysis and equips researchers with practical tools to apply it to more complex CJ studies.

<!-- 6. what is the organization of the manuscript? -->
The remainder of this manuscript is organized into five sections. @sec-theory reviews the two analytical approaches commonly applied to CJ data: the CBTL and ITCJ analyses. @sec-methods details the assumed data-generating process for the simulated dataset, the simulation procedure, the practical implementation of each analytical approach, and the evaluation criteria aligned with the research goals. @sec-results presents the data description and modeling results. @sec-discussion interprets the findings, outlines future research directions, and considers the study limitations. Finally, @sec-conclusion offers the concluding remarks.

<!-- []{style="color:red;"} -->

<!-- ######################################### -->

# A tale of two analytical approaches {#sec-theory}
<!-- Sven and Tine 2025.08.25, time: 00:12:20 - 00:14:00 -->
<!-- Sven and Tine 2025.09.04, time: 00:04:05 - 00:04:40 -->
<!-- Sven and Tine 2025.09.15, time: 00:03:30 - 00:04:20 -->

Pairwise comparison data, and more specifically CJ data, can be analyzed using two main approaches: the CBTL and the ITCJ analysis. The CBTL approach applies a sequence of separate analytical steps to estimate traits and draw inferences. In contrast, the ITCJ analysis uses a single, systematic, and integrated approach to achieve the same objectives. This section describes the two approaches in detail.


## The CBTL analysis {#sec-theory_CBTL}
<!-- Sven and Tine 2025.09.04, time: 00:04:40 - 00:07:00 -->
<!-- Sven 2025.09.29, time: 00:00:00 - 00:02:10, issues with residuals -->

The CBTL approach implements a sequence of separate analytical steps, each serving a distinct purpose [@Pollitt_2012a; @Pollitt_2012b; @Jones_et_al_2019; @Boonen_et_al_2020; @Chambers_et_al_2022; @Bouwer_et_al_2023]. This multi-step procedure typically unfolds as follows. First, analysts apply the BTL model to the CJ data to produce two outputs: (1) point estimates of stimulus traits along with their standard errors, and (2) residuals at the stimulus level. These outputs provide the foundation for the subsequent analyses.

Second, researchers summarize or fit regression models to the stimulus point estimates. This step serves multiple purposes, including aggregating stimulus-level estimates to the individual level, partitioning variability between and within individuals, and drawing inferences about factors that influence trait values. For example, @Boonen_et_al_2020 applied a multilevel regression model to the stimulus point estimates to examine whether children's age or hearing status affects their intelligibility scores.

Third, analysts summarize or fit regression models to the BTL residuals. This step helps to aggregate the remaining variability at the judge level, partition residual variability between and within judges, test for systematic biases, and identify potential misfitting judgments, stimuli, or judges. For instance, @Wu_2025 fitted an analysis of variance (ANOVA) model to the infit statistic for each rater to examine the potential effects of raters' expertise on their judgments. The infit statistics is a weighted average of the squared Pearson residuals [@Wright_et_al_1982].

While this stepwise approach is the standard practice in fields such as education [see, @Wu_2025] and linguistics [see, @Thwaites_et_al_2024], it presents several limitations. First, each stage treats outputs from previous steps as fixed data, rather than acknowledging their status as uncertain parameter estimates. Failure to account for this uncertainty can introduce bias and decrease the precision of inferences. The direction and magnitude of these biases can be unpredictable: results may be attenuated, amplified, or remain unaffected depending on the uncertainty in the scores and the actual effects being tested [@McElreath_2020; @Kline_et_al_2023; @Hoyle_et_al_2023]. Moreover, the loss of precision diminishes statistical power and increases the likelihood of committing type I or type II errors [@McElreath_2020]. Second, the procedure lacks theoretical coherence, as it combines models with different underlying assumptions. For example, third-step analyses treat residuals as outputs that ostensibly capture deviations from expected comparison outcomes, potentially reflecting judge-specific tendencies or idiosyncrasies in judgments, without providing evidence on whether these assumptions hold.


## The ITCJ analysis {#sec-theory_ITCJ}
<!-- Sven and Tine 2025.09.04, time: 00:07:00 - 00:09:40 -->

<!-- commands for d-separation -->
\newcommand{\dsep}{\:\bot\:}
\newcommand{\ndsep}{\:\not\bot\:}

The ITCJ analysis addresses the aforementioned limitations by providing a unified and systematic approach to analyzing CJ data [@Rivera_et_al_2025]. It starts with a general *Directed Acyclic Graph (DAG)* and a corresponding *Structural Causal Model (SCM)* [@Morgan_et_al_2014; @Gross_et_al_2018; @Neal_2020], which together establish a coherent theoretical foundation for CJ analysis by explicitly representing the relationships among observed judgments, discriminal differences, stimulus traits, individual traits, judge biases, and the sampling and comparison mechanisms. Next, the approach adapts the general SCM and DAG to the assumed data-generating process of the CJ system under study. Then, it derives one or more bespoke *probabilistic* and *statistical* models tailored to that system. Finally, it uses one or more statistical models to estimate traits and conduct statistical inference. @fig-ITCJ_dag illustrates an example of a general DAG structure.

![ITCJ analysis: DAG](3_results/theory/figures/png/CJ_TM_15.png){#fig-ITCJ_dag fig-align="center" fig-pos="H" width=72%}

In this representation, $O_{R}$ denotes the observed judgment outcome vector, and $D_{R}$ represents the discriminal difference vector. $T_{IA}$ captures the vector of stimulus traits, and $T_{I}$ represents the vector of individual traits. The vector of judgment biases is represented by $B_{JK}$, while the vector of judges' biases by $B_J$. Covariates at the stimulus and individual levels appear as $X_{IA}$ and $X_{I}$, respectively; while $Z_{JK}$ and $Z_{J}$ represent covariates at the judgment and judge levels. The error terms ($e_{IA}$, $e_{I}$, $e_{JK}$, $e_{J}$) capture residual variation at each level. 

The DAG also represents the sampling and comparison mechanisms as the vectors $S$ and $C$, two conditioned variables that determine how population outcomes become "observed" outcomes. Importantly, the DAG depicts $S$ and $C$ as independent from all other variables by showing no arrows pointing into them. Regarding $S$, this indicates that the DAG applies to *Simple Random Sampling (SRSg)* designs, where each repeated judgment, judge, stimulus, and individual has an equal probability of inclusion within their respective groups [@Lawson_2015]. Regarding $C$, the DAG applies to *Random Allocation Comparative Designs* [@Bramley_2015] or *Incomplete Block Designs* [@Lawson_2015], where every repeated judgment has an equal chance of being included in the sample.

Researchers can then translate this DAG representation into SCMs and probabilistic forms that express the joint distribution of a complex CJ system as a product of simpler *conditional probability distributions (CPDs)* [@Pearl_et_al_2016; @Neal_2020; see also @Rivera_et_al_2025, section 5], as illustrated in @fig-ITCJ.

::: {#fig-ITCJ layout-ncol=2 layout-valign="top"}

::: {#fig-ITCJ_scm}
$$
\begin{aligned}
  O_{R} & := f_{O}(D_{R}, S, C) \\ 
  D_{R} & := f_{D}(T_{IA}, B_{JK}) \\
  T_{IA} & := f_{T}(T_{I}, X_{IA}, e_{IA}) \\
  T_{I} & := f_{T}(X_{I}, e_{I}) \\
  B_{JK} & := f_{B}(B_{J}, Z_{JK}, e_{JK}) \\
  B_{J} & := f_{B}(Z_{J}, e_{J}) \\ \\
  e_{I} & \dsep \{ e_{J}, e_{IA}, e_{JK} \} \\
  e_{J} & \dsep \{ e_{IA}, e_{JK} \} \\
  e_{IA} & \dsep e_{JK}
\end{aligned}
$$

SCM
:::

::: {#fig-ITCJ_prob}
$$
\begin{aligned}
  & P( O_{R} \mid D_{R}, S, C ) \\
  & P( D_{R} \mid T_{IA}, B_{JK} ) \\
  & P( T_{IA} \mid T_{I}, X_{IA}, e_{IA} ) \\
  & P( T_{I} \mid X_{I}, e_{I} ) \\
  & P( B_{JK} \mid B_{J}, Z_{JK}, e_{JK} ) \\
  & P( B_{J} \mid Z_{J}, e_{J} ) \\ \\
  & P( e_{I} ) P( e_{IA} ) P( e_{J} ) P( e_{JK} ) \\ \\ \\
\end{aligned}
$$

Probabilistic model
:::

ITCJ analysis. SCM (left) and probabilistic (right) representations for DAG in @fig-ITCJ_dag.
:::

Critically, the approach allows tailoring this general structure to a specific CJ context, enabling development of parsimonious models that match the assumed data-generating process without imposing unnecessary constraints. For example, @Rivera_et_al_2025 modeled a CJ assessment designed to evaluate the impact of different teaching methods on students' writing ability. In this case, the observed outcome was binary, so the model assumed $O_{R}$ followed a Bernoulli distribution. The discriminal difference $(D_{R})$ was determined by the texts' discriminal processes $(T_{IA})$ and the judgment biases $(B_{JK})$. Student-level variables $X_{I}$, such as teaching method, were included, whereas text-level variables $X_{IA}$ (e.g., text length) were not gathered. Similarly, judge-level variables $Z_{J}$, like judgment expertise, were incorporated, while judgment-level variables $Z_{JK}$ (e.g., number of judgments per judge) were absent. Finally, the probabilistic assumptions for the idiosyncratic errors ($e_{I}$, $e_{IA}$, $e_{J}$, $e_{JK}$) resolved indeterminacies in *location*, *orientation*, and *scale* for the variables $T_{I}$, $T_{IA}$, $B_{J}$, $B_{JK}$, as required in latent variable models [@Depaoli_2021; @deAyala_2009].

Lastly, researcher can derive one or more *bespoke* statistical models tailored to the CJ system of interest, as demonstrated in @Rivera_et_al_2025. At this stage, the ITCJ analysis differs fundamentally from the CBTL approach in how it handles parameter estimation and inference. Rather than fitting multiple separate models, the approach simultaneously estimates all parameters within a single coherent framework using *Bayesian inference*. This joint estimation accounts for uncertainty at all levels and enables direct inference about quantities of interest without relying on post-hoc procedures [@McElreath_2020].


<!-- ######################################### -->

# Methods {#sec-methods}
<!-- Sven and Tine 2025.09.04, time: 00:09:40 - 00:12:25 -->
<!-- Sven and Tine 2025.09.04, time: 00:22:10 - 00:23:05 -->
<!-- Sven and Tine 2025.09.15, time: 00:04:25 - 00:11:45 -->
<!-- Sven and Tine 2025.09.04, time: 00:33:00 - 00:37:15 -->

## Step 1, from Theory to Design: Data-generating assumptions {#sec-methods_step1}
<!-- Sven and Tine 2025.09.04, time: 00:12:25 - 00:22:10 -->
<!-- Sven and Tine 2025.09.04, time: 00:39:00 - 00:48:45 -->
<!-- Sven 2025.09.29, time: 00:02:10 - 00:08:00, estimands -->

## Step 2, from Design to Data: Data simulation {#sec-methods_step2}

<!-- Sven 2025.10.31, time: 00:00:00 - 00:06:15, decision on why one data set -->

<!-- Express the whole simulation procedure with an algorithm snippet -->


## Step 5, from Estimator and Sample to Estimate(s): The analysis aproaches {#sec-methods_step5}
<!-- Sven 2025.09.29, time: 00:13:25 - 00:16:55, software -->

<!-- Tine 2025.10.24, time: 00:02:40 - 00:03:45 -->


### The CBTL analysis {#sec-methods_step5_1}
<!-- Sven and Tine 2025.09.15, time: 00:11:45 - 00:17:55, make it Bayesian -->
<!-- Sven and Tine 2025.09.15, time: 00:20:45 - 00:30:30, make it Bayesian -->
<!-- Sven and Tine 2025.09.15, time: 00:32:30 - 00:32:55 -->
<!-- Sven and Tine 2025.09.04, time: 00:37:15 - 00:38:45, issue with prediction -->
<!-- Sven and Tine 2025.09.13 p1, time: 00:08:55 - 00:17:20, modeling and variability in CBTL analysis -->
<!-- Tine 2025.10.24, time: 00:03:45 - 00:06:00 -->
<!-- Sven 2025.10.31, time: 00:06:15 - 00:09:15, same explanation as with Tine 2025.10.24 -->



### The ITCJ analysis {#sec-methods_step5_2}
<!-- Sven and Tine 2025.09.15, time: 00:17:55 - 00:20:45 -->
<!-- Sven and Tine 2025.09.15, time: 00:30:30 - 00:32:30 -->
<!-- Tine 2025.10.24, time: 00:06:00 - 00:10:40 -->
<!-- Sven 2025.10.31, time: 00:09:15 - 00:11:20, same explanation as with Tine 2025.10.24 -->
<!-- Sven 2025.10.31, time: 00:14:15 - 00:17:25, same explanation as with Tine 2025.10.24 -->

#### Model 1 {#sec-sec-methods_step5_2_itcj_1}

#### Model 2 {#sec-sec-methods_step5_2_itcj_2}

#### Model 3 {#sec-sec-methods_step5_2_itcj_3}

#### Model 4 {#sec-sec-methods_step5_2_itcj_4}

#### Model 5 {#sec-sec-methods_step5_2_itcj_5}

#### Model 6 {#sec-sec-methods_step5_2_itcj_6}


## Step 6, from Estimate(s) to Diagnostics and Posterior predictives: The evaluation criteria {#sec-methods_step6}
<!-- Sven and Tine 2025.09.04, time: 00:23:40 - 00:26:55, with two datas -->
<!-- Sven 2025.09.29, time: 00:16:55 - 00:23:40, still with two datas -->
<!-- Comment2 2025.10.17, posterior confusion matrix, with confidence intervals -->
<!-- Tine 2025.10.24, time: 00:10:40 - 00:11:50 -->
<!-- Sven 2025.10.31, time: 00:17:25 - 00:18:15, same explanation as with Tine 2025.10.24 -->


<!-- ######################################### -->

# Results {#sec-results}

## Data description {#sec-results_data}
<!-- Sven and Tine 2025.09.04, time: 00:58:10 - 01:06:20 -->
<!-- Sven 2025.09.29, time: 00:23:40 - 00:31:40 -->
<!-- Sven and Tine 2025.09.13 p1, time: 00:03:35 - 00:07:50 -->



## Data modeling {#sec-results_modeling}
<!-- Sven and Tine 25.08.25, time: 00:36:45 - 00:44:00 -->
<!-- Mention the dependence on the prior, but consider that the selected priors -->
<!-- are now determined by the CBTL analysis, so it could happen that you will -->
<!-- not need to show the dependence of the priors -->


### The CBTL analysis {#sec-results_modeling_1}
<!-- Sven 2025.09.29, time: 00:31:40 - 00:49:05, do not use all the extra variables from simulation, explanation of the issues with the residuals distribution -->

<!-- Tine 2025.10.24, time: 00:12:35 - 00:14:25 -->
<!-- Extreme cases estimation issue, are they the ones that won or loss all comparisons? -->




<!-- Tine 2025.10.24, time: 00:21:05 - 00:34:05 -->
<!-- continue the analysis without the 'misfit' stimuli -->
<!-- prior distributions for the two additional models -->
<!-- priors for the second models are more conservatives, than what the results of
Boonen reports (1 logit of difference is really high) -->

<!-- Sven 2025.10.31, time: 00:18:15 - 00:19:55, same explanation as with Tine 2025.10.24 -->

<!-- Sven 2025.10.31, time: 00:33:15 - 00:40:00, same explanation as with Tine 2025.10.24  -->




<!-- Sven 2025.09.29, time: 01:32:08 - 01:36:00, how the practice calculates residuals and WMS -->

<!-- Sven and Tine 2025.09.13 p1, time: 00:00:00 - 00:02:55, residual calculations -->

<!-- Sven and Tine 2025.09.13 p1, time: 00:17:20 - 00:19:25, wrong results!! (do not consider) -->

<!-- Sven and Tine 2025.09.13 p1, time: 00:37:55 - 00:39:00, continue modeling -->
<!-- Sven and Tine 2025.09.13 p1, time: 00:39:45 - 00:43:20, wrong results!! (do not consider) -->

<!-- Sven and Tine 2025.09.13 p1, time: 00:43:20 - 00:44:50, judges biases -->




<!-- Tine 2025.10.24, time: 00:34:05 - 00:46:40 -->
<!-- Explaining parameters recovery, RMSEs, confusion matrix, posterior predictive plots for stimuli, individuals, and influential points for judges and stimuli (as extreme stimuli) -->

<!-- Sven 2025.10.31, time: 00:32:20 - 00:33:15, same explanation as Tine 2025.10.24 -->

<!-- Sven 2025.10.31, time: 00:40:00 - 00:45:40, same explanation as Tine 2025.10.24  -->

<!-- Sven 2025.10.31, time: 00:45:40 - 00:46:30, the same influential points in CBTL analysis do not appear on the misfit analysis results (point of discussion) -->



### The ITCJ analysis {#sec-results_modeling_2}
<!-- Sven and Tine 2025.09.15, time: 00:20:45 - 00:26:00 -->
<!-- Sven and Tine 2025.09.13 p1, time: 00:44:50 - 00:54:35 -->
<!-- Sven and Tine 2025.09.13 p2, time: 00:15:25 - 00:21:30, model definitions -->
<!-- Sven and Tine 2025.09.13 p2, time: 00:24:15 - 00:25:00, model definitions -->

<!-- Tine 2025.10.24, time: 00:48:40 - 01:00:05 -->
<!-- Tine 2025.10.24, time: 01:00:45 - 01:15:50 -->
<!-- prior predictive checks and results for all ITCJ analyses: parameter recover, RMSEs, posterior predictive plots, and influential observations -->
<!-- make a point that in Bayesian models you do not need to erase extreme values, the extreme values get shrinkage towards the center just by using the priors -->

<!-- all model are build in the non-centered parametrization -->

<!-- Sven 2025.10.31, time: 00:46:30 - 01:02:40, same explanation as Tine 2025.10.24 -->


<!-- Most important part of the results: 10 percentage points increase in TP and TN compared to the CBTL analysis. -->



#### Model 1 {#sec-results_modeling_2_1}

#### Model 2 {#sec-results_modeling_2_2}

#### Model 3 {#sec-results_modeling_2_3}

#### Model 4 {#sec-results_modeling_2_4}

#### Model 5 {#sec-results_modeling_2_5}

#### Model 6 {#sec-results_modeling_2_6}

#### Model comparison {#sec-results_modeling_2_7}

<!-- Sven 2025.10.31, time: 00:00:00 - 00:00:00 -->


<!-- Tine 2025.10.24, time: 01:15:50 - 01:17:30 -->
<!-- Still considering to use a second data set to validate the model, but you have abandoned this idea already -->

<!-- Tine 2025.10.24, time: 01:22:10 - 01:24:42 -->
<!-- business idea: compare moving dots, that has an objective truths, can help you to detect Eyesights problems, where the main interest of the model is the judges biases, which in this case define the individuals that are getting the test, because the stimuli are the moving dots. -->


<!-- Sven 2025.10.31, time: 01:05:30 - 01:08:10, same explanation as Tine 2025.10.24 -->


<!-- ######################################### -->

# Discussion {#sec-discussion}

<!-- Sven and Tine 2025.09.13 p1, time: 00:07:50 - 00:08:50, summaries do not help -->

<!-- Sven and Tine 2025.09.13 p1, time: 00:08:55 - 00:12:45, modeling and variability in CBTL analysis -->

<!-- Sven and Tine 25.09.04, time: 00:27:45 - 00:00:00 -->
<!-- change the workflow from the practice -->

<!-- Sven and Tine 2025.09.04, time: 00:30:20 - 00:33:30 -->

<!-- Sven 2025.09.29, time: 00:49:05 - 01:20:30, carrying mistakes of estimations, narrow priors, and issues with misfits -->

<!-- Sven 2025.09.29, time: 01:21:55 - 01:30:45, everything perfect with the CBTL analysis, but this is not correct -->

<!-- Sven 2025.09.29, time: 01:20:30 - 01:21:55, ITCJ is better -->

<!-- Sven and Tine 2025.09.13 p1, time: 00:21:00 - 00:37:55, misfit analysis -->
<!-- Sven and Tine 2025.09.13 p1, time: 00:39:00 - 00:39:45, more misfits -->

<!-- Sven and Tine 2025.09.13 p2, time: 00:00:00 - 00:01:15, better to go complex -->

<!-- Comment 2025.10.16 and Comment 2025.10.22 -->
<!-- fit analysis maybe allows you to select model, but also you could observe the posterior distribution of certain parameters to find if the distribution favors simpler models (with informative priors centered at zero, like the horseshoe priors, but how much data do you need?). Important: the process can be done from complex model to simpler, but not the other way around, like from CBTL to complex models -->


<!-- Comment1 2025.10.17, six models are nested models, now you can assess nested hypothesis -->


<!-- Robinson (2021) pp. 3 (59) "A model is only validated with respect to its purpose. It cannot be assumed that a model that is valid for one purpose is also valid for another. For instance, a model of a production facility may have been validated for use in testing alternative production schedules, however, this does not mean that it is necessarily valid for determining that facility’s throughput. " -->

<!-- Sven and Tine 2025.09.13 p2, time: 00:01:15 - 00:04:50, model validation statement -->

<!-- Lawson (2015) p. 7 "An appropriate analysis of data cannot be completed without knowledge of what experimental design was used and how the data was collected, and conclusions are not reliable if they are not justified by the proper modelling and analysis of the data"  -->

<!-- Lawson (2015) p. 177 "In many cases where observational units are different than the experimental units, the observational units will not be independent" -->

<!-- Sven and Tine 2025.09.13 p2, time: 00:04:50 - 00:15:25, prior definition across models and interplay with sample size -->


<!-- Tine 2025.10.24, time: 00:46:40 - 00:48:35 -->
<!-- In CJ practice we cannot discard stimuli, e.g., when the the stimuli represent the work of some students. This recommendation match the statistical recommendation of not eliminating data. In the context of CBTL, the model is stuck with some observations that are extreme and misfit, but they cannot be eliminated. Ideally, this happens because the model is not appropriate, and it should be capturing more of the data-generating process of the data. -->


<!-- Tine 2025.10.24, time: 01:00:05 - 01:00:47 -->
<!-- The problem is not with CBTL analysis, the thing is that if you want to test other hypothesis not contemplated in the data-generating process that CBTL analysis represents, the way to go is NOT building two additional models with the results from the first CBTL analysis, but to modify the model to contemplate the others traits of the data-generating process. -->


<!-- Tine 2025.10.24, time: 01:17:50 - 01:18:30 -->
<!-- The idea of the black swan in modeling validation: you could make several CBTL models for this data, and still you would not fully proof that the CBTL model works, but comparing it with one simulated data set and a different model, proves that the CBTL model does not work under certain scenarios -->


<!-- Sven 2025.10.31, time: 01:02:40 - 01:05:30, 
- 25% of of FP and FN is a lot. 
- Are the models overfitting? -->


<!-- Sven 2025.10.31, time: 01:08:10 - 01:10:15, general discussion -->
<!-- Sven 2025.10.31, time: 01:11:35 - 01:13:40, general discussion -->
<!-- As an applied CJ researcher, if you have any doubts you should consider then the presence of judges biases -->

<!-- recording: Sven 2025.10.31; time: 01:24:21 - 01:31:30 -->
<!-- One point that somebody could bring is that: it could be that judges biases may cancel each other from the model. 
Notably, this assumption entails that the distribution of judges biases is symmetric around the zero axis, that is, two judges biases happen in the same magnitude but in opposite directions of the distribution. Not only that is difficult to happen, but more importantly, the way the CJ experiment takes place ensures that not all judges compare the same stimuli, and not the one stimuli is compared by all the judges. Write down the example that you make in this recording!! -->



<!-- SUPER IMPROTANT -->
<!-- all other models can be expressed as special cases of the general framework derived in @Rivera_et_al_2025 -->



## Future research directions {#sec-discussion_RD}

<!-- We are trying to do prospective power analysis, however, you should pose the possibility that other authors perform replication power (Kruschke 2013, p. 393). -->
<!-- The next paper is going to perform retrospective power analysis -->

<!-- Sven and Tine 2025.09.13 p1, time: 00:19:25 - 00:21:00, replication studies -->

<!-- Sven and Tine 2025.09.13 p2, time: 00:22:30 - 00:22:45, replication studies -->

<!-- Sven 2025.10.31, time: 01:13:40 - 01:15:55 -->


## Study limitations {#sec-discussion_limitations}

<!-- Robinson (2021) pp. 4-5 (60-61) "Indeed, it is not possible to prove that a model is valid. Instead, it is only possible to think in terms of the confidence that can be placed in a model. The process of verification and validation is not one of trying to demonstrate that the model is correct, but is in fact a process of trying to prove that the model is incorrect. The more tests that are performed in which it cannot be proved that the model is incorrect, the more the clients' (and the modeller’s) confidence in the model grows. The purpose of verification and validation is to increase the confidence in the model and its results to the point where the clients are willing to use it as an aid to decision-making." -->

<!-- Robinson (2021) pp. 6 (62) Alternatively, by expressing the code in a non-technical format a non-expert could check the data and the logic.  -->

<!-- Kruschke (2014) p. 394 "Power analysis is only useful when the simulated data imitate actual data ... We assume that the model is a reasonably good description of the actual data". 
Indeed, that is the reason our model replicates the simulated data, because if in this scenario the model cannot recover the parameters, then no other scenario would make it better at recovering -->

<!-- - model fits the data-generating process, how does the model stand when we want to test hypothesis about a particular data-generating process  -->



<!-- - How strong is the evidence in favor of judges biases. We can solve this from two Bayesian perspectives: from the parameter estimation and the model selection perspective [@Wagenmakers_2014, pp. 178] -->

<!-- Sven and Tine 2025.09.13 p2, time: 00:23:25 - 00:24:00, Bayesian perspective for hypothesis testing -->


<!-- Collett (2023) pp. 425 "The difficulty of specifying prior information in an accurate and non-controversial manner, coupled with a desire to use priors that have little or no influence on the final inference, means that non-informative priors are widely used. However, the use of a flat prior distribution to represent lack of prior knowledge, can be criticized. For example, suppose that a hazard ratio for a treatment effect is a key parameter. A flat prior that is uniform on (0,∞) could be used to express prior ignorance about the value of the hazard ratio. However, this is saying that our prior information is such that a hazard ratio in the range from 0 to 10 is just as plausible as a hazard ratio in the range from 1000 to 1010. This is rather unlikely to be the case. -->
<!-- ... -->
<!-- Although different prior distributions will necessarily lead to different posterior distributions, any such differences may not be of practical importance in terms of inferences to be drawn. ... This is particularly the case for larger datasets, but differences may be more noticeable in smaller ones." -->


<!-- Collett (2023) pp. 426 "There is always some degree of arbitrariness when choosing a prior distribution, and there is often concern about the extent to which a posterior distribution is dependent on the choice that is made. For this reason, a range of plausible priors can be used in an analysis, and the sensitivity of the posterior distribution to these different choices evaluated." -->


<!-- Problem: Shrinked parameter estimates -->
<!-- You need to check: -->
<!-- 1. Priors for the parameters, current are normal(0,0.3). This might be too tight shrinking parameters a lot. Maybe you need to increase it to normal(0,1). (THIS IS THE ONE!) -->
<!-- 2. Remember that the current estimates are not corrected by the sampling probability of the groups (sample size vs population size). Maybe this also impacts the parameter estimates. -->



<!-- Improving sampling parameters: -->
<!-- Sample all parameters (individuals, stimuli, judges and judgments) using: -->

<!-- 1. The assumption that parameters are not correlated:  -->
<!-- - beta ~ multi_normal( rep(0, P), sigma*diag(1,P)*sigma) -->
<!-- - length(sigma) = P (number of parameters) -->

<!-- Pros: this you can do because you directly assume the absence of correlation and a particular sigma among parameters -->
<!-- Cons: how valid is the assumption of no correlation and the sigmas? -->


<!-- Sven 2025.10.31, time: 01:15:55 - 01:19:55 -->
<!-- Why downsize your own evidence? why do you say it is preliminary?, it is preliminary until I do prospective power analysis or the actual mathematical derivation. Because it could happen that by random chance i managed to simulate a data that satisfies the conditions of the model. However, despite this, I am leaving my replicable code in such way that any other researcher can simulate another data set, and check if that new data set also recovers well the parameters and achieves good fit compared to the CBTL analysis, that is, a concept like a crowd sourcing the prospective power analysis. -->
<!-- When you mention the limitation regarding sample size, "to cover this limitation, all the code is available ... " -->



<!-- ######################################### -->

# Conclusion {#sec-conclusion}

<!-- Sven 2025.10.31, time: 01:20:45 - 01:21:25, Conclusions -->


<!-- ######################################### -->

{{< pagebreak >}}

# Declarations {.unnumbered .appendix appendix-style=plain}

**Funding:** The Research Fund (BOF) of the University of Antwerp funded this project.

**Financial interests:** The authors declare no relevant financial interests.

**Non-financial interests:** The authors declare no relevant non-financial interests.

**Ethics approval:** The University of Antwerp Research Ethics Committee confirmed that this study does not require ethical approval.

**Consent to participate:** Not applicable

**Consent for publication:** All authors have read and approved the final version of the manuscript for publication.

**Data, materials and code availability:** A previous version of this manuscript, along with the associated data, materials and code (see the section titled `CODE LINK`), has been made publicly available at: [https://jriveraespejo.github.io/paper3_manuscript/](https://jriveraespejo.github.io/paper3_manuscript/). 

**Licence:** All the code that is original to this study and not attributed to any other authors is copyrighted by [Jose Manuel Rivera Espejo](https://orcid.org/0000-0002-3088-2783) and released under the new [BSD-3-Clause](https://opensource.org/license/BSD-3-Clause) license. 

**AI-assisted technologies in the writing process:** The authors used various AI-based language tools to refine phrasing, optimize wording, and enhance clarity and coherence throughout the manuscript. They take full responsibility for the final content of the publication.

**CRediT authorship contribution statement:** *Conceptualization:* J.M.R.E, T.vD., S.DM., and S.G.; *Methodology:* J.M.R.E, T.vD., and S.DM.; *Software:* J.M.R.E.; *Validation:* J.M.R.E.; *Formal Analysis:* J.M.R.E.; *Investigation:* J.M.R.E; *Resources:* T.vD. and S.DM.; *Data curation:* J.M.R.E.; *Writing - original draft:* J.M.R.E.; *Writing - review and editing:* J.M.R.E., T.vD., S.DM., and S.G.; *Visualization:* J.M.R.E.; *Supervision:* S.G. and S.DM.; *Project administration:* S.G. and S.DM.; *Funding acquisition:* S.G. and S.DM.


<!-- **Acknowledgements:** -->


<!-- ######################################### -->

{{< pagebreak >}}

# Appendix {#sec-appendix .appendix appendix-style=plain}

## Appendix A: Stationarity, converge and mixing {#sec-appendixA .appendix appendix-style=plain}



## Appendix B: Misfit observations {#sec-appendixB .appendix appendix-style=plain}
<!-- Tine 2025.10.24, time: 00:14:25 - 00:21:05 -->
<!-- Misfit stimuli are not necessarily extreme values, but the conceptual understanding of misfit does not match with the operationalization -->
<!-- Data is not simulated with misfits, so what is the tool identifying? -->
<!-- Tine's explanation of what you call a "preference analysis" as in Figure 24 -->

<!-- Sven 2025.10.31, time: 00:19:55 - 00:32:20, The problematic of misfit statistics. Due to the data-generating process, which includes judges biases, lack of transitivity is a common trait of the data. Thus, the question remains: what does the misfit analysis identifies, if lack of transitivity is a common trait of comparative judgment data? -->

<!-- Sven 2025.10.31, time: 00:32:20 - 00:33:15, does the practice of eliminating stimuli is justified? -->

<!-- Sven 2025.10.31, time: 01:10:15 - 01:11:35, misfits and lack of transitivity -->

<!-- Sven 2025.10.31, time: 01:19:55 - 01:00:00, "despite the interesting result regarding misfit analysis, the study did not explicitly design or plan the misfit analysis properly. Thus, more research should be done in this regard." -->


<!-- recording: Sven 2025.10.31; time: 01:22:35 - 01:24:21 -->
<!-- It is not the end of the study. But what I'm trying to do is to change the practice. -->
<!-- Let's assume misfit analysis has its value, how can you be certain that the method is useful, using the wrong model? If you have a slight doubt that your data has judges biases, how can you trust an analysis that does not consider judges biases in the model? -->


## Appendix C: Sample size calculations {#sec-appendixC .appendix appendix-style=plain}
<!-- Sven and Tine 2025.09.04, time: 00:48:45 - 00:58:10 -->
<!-- Sven 2025.09.29, time: 00:08:00 - 00:13:25 -->
<!-- Tine 2025.10.24, time: 00:00:00 - 00:02:40 -->


<!-- ######################################### -->

{{< pagebreak >}}

# References {.unnumbered .appendix appendix-style=plain} 

:::{#refs}

:::
